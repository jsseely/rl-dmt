{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc065151",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install minigrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "\n",
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=10,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            # Set this to True for maximum speed\n",
    "            see_through_walls=True,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"grand mission\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        # Create an empty grid\n",
    "        self.grid = Grid(width, height)\n",
    "\n",
    "        # Generate the surrounding walls\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Generate verical separation wall\n",
    "        for i in range(0, height):\n",
    "            self.grid.set(5, i, Wall())\n",
    "        \n",
    "        # Place the door and key\n",
    "        self.grid.set(5, 6, Door(COLOR_NAMES[0], is_locked=True))\n",
    "        self.grid.set(3, 6, Key(COLOR_NAMES[0]))\n",
    "\n",
    "        # Place a goal square in the bottom-right corner\n",
    "        self.put_obj(Goal(), width - 2, height - 2)\n",
    "\n",
    "        # Place the agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"grand mission\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = SimpleEnv(render_mode=\"human\")\n",
    "\n",
    "    # enable manual control for testing\n",
    "    manual_control = ManualControl(env, seed=42)\n",
    "    manual_control.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22496d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_control = ManualControl(env, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from minigrid.wrappers import ImgObsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ImgObsWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_control = ManualControl(env, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87cda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_control.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb93407",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_control.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca03cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_control.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f55689",
   "metadata": {},
   "source": [
    "# train a ppo agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0471ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 512, normalized_image: bool = False) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a358c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87141ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minigrid\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MinigridFeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "\n",
    "env = gym.make(\"MiniGrid-Empty-16x16-v0\", render_mode=\"rgb_array\")\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(2e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c926c",
   "metadata": {},
   "source": [
    "# simple mdp from gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self, size=5, stochastic=False):\n",
    "        self.size = size\n",
    "        self.stochastic = stochastic\n",
    "        self.states = np.arange(size * size).reshape((size, size))\n",
    "        self.values = np.zeros((size, size))\n",
    "        self.policy = np.ones((size, size, 4)) / 4  # Uniform random policy\n",
    "        self.terminal_states = [(size-1, size-1)]\n",
    "        self.action_space = ['up', 'down', 'left', 'right']\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        if state in self.terminal_states:\n",
    "            return state, 0\n",
    "        \n",
    "        next_state = list(state)\n",
    "        if action == 'up':\n",
    "            next_state[0] = max(0, state[0] - 1)\n",
    "        elif action == 'down':\n",
    "            next_state[0] = min(self.size - 1, state[0] + 1)\n",
    "        elif action == 'left':\n",
    "            next_state[1] = max(0, state[1] - 1)\n",
    "        elif action == 'right':\n",
    "            next_state[1] = min(self.size - 1, state[1] + 1)\n",
    "        \n",
    "        if self.stochastic and np.random.rand() < 0.1:\n",
    "            next_state = [np.random.randint(self.size), np.random.randint(self.size)]\n",
    "        \n",
    "        return tuple(next_state), -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self):\n",
    "        self.state_inds = list(range(10))\n",
    "        self.state_vals = [f\"S{i}\" for i in self.state_inds]\n",
    "        self.actions = {i: [f\"A{j}\" for j in range(3)] for i in self.state_inds}\n",
    "        self.rewards = [-1 for i in self.state_inds]\n",
    "        \n",
    "        self.terminal_states = [\"S0\"]\n",
    "        self.state = \"S8\"\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        \n",
    "    def transition(self, action):\n",
    "        # formally:\n",
    "        # takes input state and action\n",
    "        # produces a s x r grid (all possible next states, all possible rewards)\n",
    "        # in that grid we have real values\n",
    "        # sum of those values must be 1.\n",
    "        # sample to get the next state and reward\n",
    "        \n",
    "        \n",
    "        # Currently the transition dynamics are deterministic\n",
    "        state = self.state\n",
    "        state_ind = self.state_vals.index(state)\n",
    "        # assert action is in A(s)\n",
    "        if action == \"A0\":\n",
    "            next_state_ind = max(state_ind - 1, 0)\n",
    "        elif action == \"A1\":\n",
    "            next_state_ind = state_ind\n",
    "        elif action == \"A2\":\n",
    "            next_state_ind = min(state_ind + 1, len(self.state_inds)-1)\n",
    "        \n",
    "        reward = self.rewards[next_state_ind]\n",
    "        next_state = self.state_vals[next_state_ind]\n",
    "        \n",
    "        # iterate:\n",
    "        self.state = next_state\n",
    "        self.total_reward += reward\n",
    "                \n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f60332",
   "metadata": {},
   "source": [
    "# policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticPolicy:\n",
    "    def __init__(self, mdp_instance):\n",
    "        \"\"\"\n",
    "        mdp_instance: MDP\n",
    "            An instance of the MDP class.\n",
    "        \"\"\"\n",
    "        self.policy = {}\n",
    "        \n",
    "        # Iterate over all states in the MDP\n",
    "        for state in mdp_instance.state_vals:\n",
    "            # Correcting the key access to use integers for the actions dictionary\n",
    "            state_index = mdp_instance.state_vals.index(state)\n",
    "            actions = mdp_instance.actions[state_index]\n",
    "            num_actions = len(actions)\n",
    "            \n",
    "            # Assign equal probabilities to each action for the state\n",
    "            # action_probabilities = {action: 1.0 / num_actions for action in actions}\n",
    "            # action_probabilities = {\"A0\": 0.9, \"A1\": 0.05, \"A2\": 0.05}\n",
    "            action_probabilities = {\"A0\": 1.0, \"A1\": 0.0, \"A2\": 0.0}\n",
    "            self.policy[state] = action_probabilities\n",
    "    \n",
    "    def get_action(self, state, rng=None):\n",
    "        \"\"\"\n",
    "        Samples and returns an action for the given state based on the policy probabilities.\n",
    "        \"\"\"\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "        \n",
    "        actions = list(self.policy[state].keys())\n",
    "        probabilities = list(self.policy[state].values())\n",
    "        \n",
    "        return rng.choice(actions, p=probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a590c72",
   "metadata": {},
   "source": [
    "# simulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = MDP()\n",
    "\n",
    "policy = StochasticPolicy(world)\n",
    "\n",
    "step = 0\n",
    "while world.state not in world.terminal_states:\n",
    "    step += 1\n",
    "    state = world.state\n",
    "    action = policy.get_action(state)\n",
    "    next_state, reward = world.transition(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661db70f",
   "metadata": {},
   "source": [
    "# value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff75ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunctions:\n",
    "    def __init__(self, mdp_instance, policy_instance):\n",
    "        self.V = {state: 0.1.*np.random.randn() for state in mdp_instance.state_vals}\n",
    "        self.mdp = mdp_instance\n",
    "        self.policy = policy_instance\n",
    "        \n",
    "    def policy_evaluation(self, theta=1e-9, max_iterations=100):\n",
    "        \"\"\"\n",
    "        Perform policy evaluation to estimate the value function V(s) for the given policy.\n",
    "        \n",
    "        :param theta: float, threshold for determining convergence\n",
    "        :param max_iterations: int, maximum number of iterations to prevent infinite loops\n",
    "        :return: None, updates the V attribute in place\n",
    "        \"\"\"\n",
    "        for iteration in range(max_iterations):\n",
    "            delta = 0  # To check for convergence\n",
    "            for state in self.V:\n",
    "                if state in self.mdp.terminal_states:\n",
    "                    continue  # Skip update for terminal states\n",
    "                v = self.V[state]\n",
    "                \n",
    "                # Save current state\n",
    "                current_state = self.mdp.state\n",
    "                \n",
    "                # Set MDP state to current state for evaluation\n",
    "                self.mdp.state = state\n",
    "                \n",
    "                # Get action probabilities from policy\n",
    "                action_probs = self.policy.policy.get(state, {})\n",
    "                \n",
    "                # Update V(s) based on expected return\n",
    "                new_v = 0\n",
    "                for action, action_prob in action_probs.items():\n",
    "                    next_state, reward = self.mdp.transition(action)\n",
    "                    \n",
    "                    # Update value function\n",
    "                    new_v += action_prob * (reward + self.mdp.gamma * self.V[next_state])\n",
    "                    \n",
    "                    # Revert MDP state for next calculations\n",
    "                    self.mdp.state = state\n",
    "                \n",
    "                # Update V(s) and track maximum change for convergence check\n",
    "                self.V[state] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "                \n",
    "                # Restore the original state\n",
    "                self.mdp.state = current_state\n",
    "            \n",
    "            # Check for convergence\n",
    "            if delta < theta:\n",
    "                print(f'Policy Evaluation converged in {iteration + 1} iterations.')\n",
    "                break\n",
    "        else:\n",
    "            print('Policy Evaluation reached maximum iterations without convergence.')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f11c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value functions\n",
    "value_functions = ValueFunctions(world, policy)\n",
    "\n",
    "# Perform policy evaluation\n",
    "value_functions.policy_evaluation()\n",
    "\n",
    "# Show resulting value function\n",
    "value_functions.V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value functions\n",
    "value_functions = ValueFunctions(world, policy)\n",
    "\n",
    "# Perform policy evaluation\n",
    "value_functions.policy_evaluation()\n",
    "\n",
    "# Show resulting value function\n",
    "value_functions.V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_functions.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25810044",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(value_functions.V.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d0fea",
   "metadata": {},
   "source": [
    "# as gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977535e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4192f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CustomMDP(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Discrete(10)\n",
    "        self.state = 8\n",
    "        self.terminal_states = [0]\n",
    "        self.gamma = 1.0\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.state in self.terminal_states:\n",
    "            return self.state, 0, True, {}  # Terminal state, no reward, episode is done\n",
    "        \n",
    "        if action == 0:  # \"A0\"\n",
    "            next_state = max(self.state - 1, 0)\n",
    "        elif action == 1:  # \"A1\"\n",
    "            next_state = self.state\n",
    "        elif action == 2:  # \"A2\"\n",
    "            next_state = min(self.state + 1, 9)\n",
    "        \n",
    "        reward = -1\n",
    "        self.state = next_state\n",
    "        self.total_reward += reward\n",
    "        done = self.state in self.terminal_states\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.state = 8\n",
    "        self.total_reward = 0\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"State: {self.state}, Total Reward: {self.total_reward}\")\n",
    "\n",
    "env = CustomMDP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98371253",
   "metadata": {},
   "outputs": [],
   "source": [
    "??env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c7c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 8, Total Reward: -1\n",
      "State: 9, Total Reward: -2\n",
      "State: 8, Total Reward: -3\n",
      "State: 8, Total Reward: -4\n",
      "State: 7, Total Reward: -5\n",
      "State: 6, Total Reward: -6\n",
      "State: 6, Total Reward: -7\n",
      "State: 7, Total Reward: -8\n",
      "State: 8, Total Reward: -9\n",
      "State: 8, Total Reward: -10\n",
      "State: 8, Total Reward: -11\n",
      "State: 9, Total Reward: -12\n",
      "State: 9, Total Reward: -13\n",
      "State: 9, Total Reward: -14\n",
      "State: 9, Total Reward: -15\n",
      "State: 9, Total Reward: -16\n",
      "State: 9, Total Reward: -17\n",
      "State: 9, Total Reward: -18\n",
      "State: 9, Total Reward: -19\n",
      "State: 8, Total Reward: -20\n",
      "State: 8, Total Reward: -21\n",
      "State: 7, Total Reward: -22\n",
      "State: 7, Total Reward: -23\n",
      "State: 7, Total Reward: -24\n",
      "State: 8, Total Reward: -25\n",
      "State: 9, Total Reward: -26\n",
      "State: 9, Total Reward: -27\n",
      "State: 9, Total Reward: -28\n",
      "State: 9, Total Reward: -29\n",
      "State: 8, Total Reward: -30\n",
      "State: 9, Total Reward: -31\n",
      "State: 9, Total Reward: -32\n",
      "State: 8, Total Reward: -33\n",
      "State: 9, Total Reward: -34\n",
      "State: 9, Total Reward: -35\n",
      "State: 9, Total Reward: -36\n",
      "State: 8, Total Reward: -37\n",
      "State: 9, Total Reward: -38\n",
      "State: 8, Total Reward: -39\n",
      "State: 8, Total Reward: -40\n",
      "State: 9, Total Reward: -41\n",
      "State: 9, Total Reward: -42\n",
      "State: 9, Total Reward: -43\n",
      "State: 9, Total Reward: -44\n",
      "State: 8, Total Reward: -45\n",
      "State: 9, Total Reward: -46\n",
      "State: 9, Total Reward: -47\n",
      "State: 9, Total Reward: -48\n",
      "State: 9, Total Reward: -49\n",
      "State: 9, Total Reward: -50\n",
      "State: 8, Total Reward: -51\n",
      "State: 7, Total Reward: -52\n",
      "State: 7, Total Reward: -53\n",
      "State: 7, Total Reward: -54\n",
      "State: 8, Total Reward: -55\n",
      "State: 8, Total Reward: -56\n",
      "State: 7, Total Reward: -57\n",
      "State: 7, Total Reward: -58\n",
      "State: 7, Total Reward: -59\n",
      "State: 8, Total Reward: -60\n",
      "State: 7, Total Reward: -61\n",
      "State: 7, Total Reward: -62\n",
      "State: 6, Total Reward: -63\n",
      "State: 6, Total Reward: -64\n",
      "State: 7, Total Reward: -65\n",
      "State: 8, Total Reward: -66\n",
      "State: 9, Total Reward: -67\n",
      "State: 9, Total Reward: -68\n",
      "State: 9, Total Reward: -69\n",
      "State: 9, Total Reward: -70\n",
      "State: 8, Total Reward: -71\n",
      "State: 7, Total Reward: -72\n",
      "State: 6, Total Reward: -73\n",
      "State: 5, Total Reward: -74\n",
      "State: 5, Total Reward: -75\n",
      "State: 4, Total Reward: -76\n",
      "State: 4, Total Reward: -77\n",
      "State: 3, Total Reward: -78\n",
      "State: 2, Total Reward: -79\n",
      "State: 3, Total Reward: -80\n",
      "State: 2, Total Reward: -81\n",
      "State: 3, Total Reward: -82\n",
      "State: 3, Total Reward: -83\n",
      "State: 4, Total Reward: -84\n",
      "State: 4, Total Reward: -85\n",
      "State: 4, Total Reward: -86\n",
      "State: 3, Total Reward: -87\n",
      "State: 4, Total Reward: -88\n",
      "State: 4, Total Reward: -89\n",
      "State: 4, Total Reward: -90\n",
      "State: 3, Total Reward: -91\n",
      "State: 2, Total Reward: -92\n",
      "State: 1, Total Reward: -93\n",
      "State: 0, Total Reward: -94\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "\n",
    "# Define the agent (policy)\n",
    "agent = sb3.PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "agent.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = agent.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb52ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae265eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy.evaluate_actions(torch.tensor(5).float().unsqueeze(0), torch.tensor(0).float().unsqueeze(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "value_estimates = []\n",
    "\n",
    "for state in range(env.observation_space.n):\n",
    "    obs_tensor = torch.tensor([state]).float().unsqueeze(0)\n",
    "    # You can pass dummy actions since they are not used for value estimation\n",
    "    actions = torch.tensor([0]).float().unsqueeze(0)\n",
    "    value, _, _ = agent.policy.evaluate_actions(obs_tensor, actions)\n",
    "    value_estimates.append(value.item())\n",
    "\n",
    "# Print the value function estimates\n",
    "for state, value in enumerate(value_estimates):\n",
    "    print(f\"V(S{state}) = {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6b991b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c7ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = spaces.Discrete(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "649134bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Discrete' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jseely/Documents/Python/rlpg/notebooks/whiteboard.ipynb Cell 42\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jseely/Documents/Python/rlpg/notebooks/whiteboard.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m obs(\u001b[39m0\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Discrete' object is not callable"
     ]
    }
   ],
   "source": [
    "obs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d2b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
